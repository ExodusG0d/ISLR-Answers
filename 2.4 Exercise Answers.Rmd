---
title: "2.4 Exercise Answers"
author: "Zou Junzhe"
date: "2024-07-09"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Conceptual

## Q1

For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

(a) *The sample size n is extremely large, and the number of predictors p is small.*

Answer: flexible learning method would perform **better** because sample size is large enough to fit more parameters and small number of predictors limits model variance.

Explain:The more parameters you want to fit, the larger your sample size needs to be. So it's possible to fit more parameters especially your sample size is far more larger than your predictors which means more flexibility.

(b) *The number of predictors p is extremely large, and the number of observations n is small.*

Answer: flexible method would perform **worse** because it would be more likely to overfit.

Explain: the ratio of predictors and sample size is big. The felxible method with more parameters may have really high weight on every single observation of the very few samples. If there is an anomalous data point, the model would perform extremely bad.

(c) *The relationship between the predictors and response is highly non-linear.*

Answer:flexible learning method would perform **better** because it is less restrictive on the shape of fit

(d) *The variance of the error terms, i.e. σ2 = Var(ε), is extremely high.*

Answer: flexible learning method would perform **worse** because it would be more likely to overfit.

Explain: high $\epsilon$ means that the data point is more likely to be anomalous.

Notes: MSE evaluates a model. And MSE contains Variance and Bias. Variance indicates how significantly the model would change with a change of training data set. Bias is more like the difference between the estimate value and real value. A more flexible model( a curve passes through every single training observation) usually has low bias but high variance. Because you change one point your model change with the point and that's variance. A more inflexible model( a horizontal line) has low variance but higher bias. The horizontal model barely passes through any observation which indicates high bias but also indicates that the variance would be low.

## Q2

Explain whether each scenario is a classification or regression prob- lem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

(a) *We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.*

Answer:

-   regression
-   inference
-   observations: n = 500
-   predictors: p = 3 (profit,number of employees, industry)

(b) *We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.*

Answer:

-   classification
-   prediction
-   observations: n = 20
-   predictors: p = 13 (price charged for the product, marketing budget, competition price, and ten other variables)

(c) *We are interesting in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market.*

Answer:

-   regression
-   prediction
-   observations: n = 52 $52 = 364 / 2$
-   predictors: p = 3 (the % change in the US market, the % change in the British market, and the % change in the German market)

Notes:

> We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems.

## Q3

We now revisit the bias-variance decomposition.

(a) *Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.*

Answer:

(b) *Explain why each of the five curves has the shape displayed in part (a).*

Answer:

-   train error: train error will continuously decreases to zero with increasing amount of flexibility.
-   test error: At first, test error decrease because a larger amount of flexibility makes the model fits better. But at some point, the problem of overfitting starts to have a role, which makes test error start to increase.
-   variance: it continuously increases with flexibility because of its definition.
-   irreducible error: irreducible error doesn't relates to the flexibility.
-   bias: bias is kind of similar to training error. It continuously decreases as flexibility increases.

## Q4

You will now think of some real-life applications for statistical learning.

(a) *Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.*

Answer:

-   1

-   University application.

-   responses: admit or reject

-   predictors: GPA, Internship, Academic research

-   prediction

-   2

-   Client classification

-   responses : VIP, Members, Ordinary

-   predictors : monthly consumption, family reputation, Occupation

(b) *Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.*

-   1
-   store's sales
-   response: a store's sales
-   predictors: location, customer traffic, opening hours
-   prediction
-   2
-   salary
-   response: salary
-   predictors: job title, length of service, education
-   prediction

(c) Describe three real-life applications in which cluster analysis might be useful.

-   1
-   type of residents
-   type of cancer cells

Note: There's one point that clustering clusters the original data set but classification makes predictions. Classification is supervised learning and clustering is unsupervised. Cluster analysis results in clusters of observations without predefined labels.

## Q5

What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

Answer:

For a very flexible approach:

-   advantages: better fits the data; fewer assumptions
-   disadvantages: hard to interpret; overfitting

Circumstances for flexible approach: the data itself is so complex that it needs more parameters; we care about the prediction instead of inference.

Circumstances for less flexible approaches: the data is simple and it attaches importance to inference.

## Q6

Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a para- metric approach to regression or classification (as opposed to a non- parametric approach)? What are its disadvantages?

Answer:

-   the difference: parametric statistical learning makes assumptions on the shape of the model but non-parametric approach makes no assumptions on that.
-   parametric approach's advantages and disadvantages: parametric approach needs less data by making assumptions on the shape of the model, which turns out to be a disadvantage that the assumption may be wrong at first. Non-parametric approach requires more data but avoids the possible mistake of making wrong assumptions.

## Q7

(a) *Compute the Euclidean distance between each observation and the test point,X1 =X2 =X3 =0.*

Answer:

```{r}
ob0 = c(0,0,0)
ob1 = c(0,3,0)
ob2 = c(2,0,0)
ob3 = c(0,1,3)
ob4 = c(0,1,2)
ob5 = c(-1,0,1)
ob6 = c(1,1,1)

dist1 = sqrt(sum((ob1 - ob0)^2))
dist2 = sqrt(sum((ob2 - ob0)^2))
dist3 = sqrt(sum((ob3 - ob0)^2))
dist4 = sqrt(sum((ob4 - ob0)^2))
dist5 = sqrt(sum((ob5 - ob0)^2))
dist6 = sqrt(sum((ob6 - ob0)^2))

print(paste(dist1,dist2,dist3,dist4,dist5,dist6))
```

(b) *What is our prediction with K = 1? Why?*

Answer:

K=1 then include ob5(Green). So prediction = **Green**

(c) *What is our prediction with K = 3? Why?*

Answer:

K=3, then include ob5(Green), ob6(Red) and ob2(Red). So prediction = **Red**.

(d) *If the Bayes decision boundary in this problem is highly non-linear, then would we expect the best value for K to be large or small? Why?*

Answer:

We expect K to be smaller to capture the non-linear part of the boundary.

Note: If K is big, the KNN boundary may be flat and ignore the irregularity on the edge. Just like the example of K = 1 and K = 100 in the main part.

# Applied

## Q8

(a) *Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.*

you can download the data set here: <https://www.statlearning.com/resources-second-edition>

```{r}
college = read.csv("College.csv")
str(college)

library(ISLR2)
data(College)
head(College)
```

(b) *Look at the data using the fix() function.*

the code explains how to make the first column a "row.names" "column" which is not in the data set

```{r}
head(rownames(college)) # original row names of the data is the order number  
```

```{r}
rownames(college)=college[,1] # we use the first column's value to be the "college"'s row names
#fix(college) #there's an identical column named "row.names" to the first column in the original data set
```

```{r}
college=college[,-1]
#fix(college) # you can see that the "row.names" column didn't get removed
```

(c) 

<!-- -->

i.  *Use the summary() function to produce a numerical summary of the variables in the data set.*

```{r}
summary(college)
```

ii. *Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10].*

```{r}
pairs(college[,2:10]) # the first column "Private" is not numeric
```

iii. *Use the plot() function to produce side-by-side boxplots of Outstate versus Private.*

```{r}
boxplot(college$Outstate~college$Private,xlab = "Private", ylab = "Outstate")
```

iv. *Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.*

```{r}
Elite = rep("No", nrow(college)) # you first create a all "No" vector with the same length as college's rows
Elite[college$Top10perc > 50] = "Yes" #Then you change the label for those whose Top10perc > 50
Elite = as.factor(Elite) #Change to qualitative variable
college = data.frame(college,Elite) #bind the new variable into the data frame
```

```{r}
summary(Elite)
plot(Outstate ~ Elite,data = college, xlab = "Elite", ylab = "Outstate")
```

Note:

*Elite[college\$Top10perc \> 50] = "Yes"* is a conditional value change sentence(forgive my poor English). *college\$Top10perc \> 50 will return a TRUE and FALSE sequence indicates the corresponding position to make the change or not. If "TRUE", the same position of* ELite\* will be changed into Yes.

v.  *Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.*

```{r}
par(mfrow=c(2,2))
hist(college$Apps, breaks=100,xlim = c(0,25000), main="Apps")
hist(College$Enroll, breaks=25, main="Enroll")
hist(College$Expend, breaks=25, main="Expend")
hist(College$Outstate, main="Outstate")
```

## Q9

```{r}
auto = Auto
auto = na.omit(auto) #remove the missing values
```


(a) *Which of the predictors are quantitative, and which are qualitative?*

```{r}
str(auto)
summary(auto)
```

quantitative: mpg, displacement, horsepower, weight, acceleration, year

qualitative: cylinder, origin, name

(b) *What is the range of each quantitative predictor? You can answer this using the range() function.*

```{r}
range(auto$mpg)
range(auto$displacement)
range(auto$horsepower)
range(auto$weight)
range(auto$acceleration)
range(auto$year)
```


(c) *What is the mean and standard deviation of each quantitative predictor?*

```{r}
sapply(auto[,c(1,3,4,5,6,7)], mean)
sapply(auto[,c(1,3,4,5,6,7)], sd)
```

Note: sapply is a function to do a certain function on several list at one time and tries to simplify the result.

(d) *Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?*

```{r}
tem = auto[,-(8:9)] # drop origin, name
less_auto = tem[-(10:85),] # remove the specified rows
sapply(less_auto[,c(1,3,4,5,6,7)],range)
sapply(less_auto[,c(1,3,4,5,6,7)], mean)
sapply(less_auto[,c(1,3,4,5,6,7)],sd)
```

Note: when dropping multiple cols or rows, need "()" like [-(10:85),] and [-10:85,] is wrong.

(e) *Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.*

```{r}
pairs(auto[,c(1,3,4,5,6,7)])
```

eg: horsepower is increasing with weight.

(f) *Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.*

eg: mpg may have a negative relation with horsepower.

## Q10

(a) *To begin, load in the Boston data set. The Boston data set is part of the MASS library in R.*

```{r}
library(MASS)
?Boston
```

(b) *Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.*

```{r}
str(Boston) 
pairs(Boston)
```

eg: age has a negative relation with dis. It may indicates that the employment centers may have more old buildings.

(c) *Are any of the predictors associated with per capita crime rate? If so, explain the relationship.*

```{r}
plot(Boston$crim,Boston$medv)
```

eg: higher medv area has significantly low crim


(d) *Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.*

```{r}
require(ggplot2)
g <- ggplot(Boston, aes(x=1:nrow(Boston), y=crim))
g + geom_point()
g <- ggplot(Boston, aes(x=1:nrow(Boston), y=tax))
g + geom_point()
g <- ggplot(Boston, aes(x=1:nrow(Boston), y=ptratio))
g + geom_point()
```

crime has outliers; tax has outliers; pupil-teacher ratio might have outliers.

(e) *How many of the suburbs in this data set bound the Charles river?*

```{r}
sum(Boston$chas) 
#chas
#Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
```

35 towns set bound the Charls river.

(f) *What is the median pupil-teacher ratio among the towns in this data set?*

```{r}
median(Boston$ptratio)
```

median ptratio is 19.05

(g) *Which suburb of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your findings.*

```{r}
lowest <- Boston[Boston$medv == min(Boston$medv),] # find the lowest medv rows
lowest
```

```{r}
summary(Boston) # the overall ranges of predictors
```

findings: they have higher *crim* than other suburbs. their *age* and *rad* are the maximum level.

(h) *In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.*

```{r}
nrow(Boston[Boston$rm > 7,]) # number of dwellings with 7+ rooms
nrow(Boston[Boston$rm > 8,]) # number of dwellings with 8+ rooms
```

64 has more than 7 rooms and 13 has more than 8 rooms.

```{r}
room_7 <- Boston[Boston$rm > 7 , ]
room_8 <- Boston[Boston$rm > 8 , ] # let them be a new data set
sapply(room_7, mean)
sapply(room_8, mean)
sapply(Boston[Boston$rm <= 7,],mean)
sapply(Boston[Boston$rm <= 8,],mean) # if you want to do a function to all the variables simultaneously, you use "sapply"
```

eg: *crim* is significantly lower than others.